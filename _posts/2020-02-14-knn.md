---
layout: post
title:  "k-최근접 이웃(k-NN)"
subtitle:   ""
categories: data_science
tags: machine_learning
mathjax: true
comments: true
---

k-NN은 복잡한 수식이 필요없는 매우 간단한 지도학습 알고리즘이다. 지도학습은 분류(Classifier)과 회귀(Regressor)로 나뉘는데 k-NN은 분류와 회귀 둘다 가능하다.

# 1. k-NN(K-Nearest Neighbor)

k-NN은 예측하고 싶은 데이터로부터 가장 가까운 $k$개의 훈련데이터를 뽑아 종속변수의 값을 예측하게 된다. 가까운 정도는 **유클리디안 거리**를 이용한다.

두 데이터 $x_{1}$, $x_{2}$가 있을 때, 두 데이터 사이의 거리 $d_{euclidean}$는 다음과 같이 표현할 수 있다.

$x_{1} = (x_{11}, x_{12}, \cdots, x_{1n})$, $x_{2} = (x_{21}, x_{22}, \cdots, x_{2n})$ 라 하자.

$$
\begin{aligned}
d_{euclidean} & = \sqrt{(x_{11} - x_{21})^2 + (x_{12} - x_{22})^2 + \cdots + (x_{1n} - x_{2n})^2} \\
& = (\sum_{i}^{n} \left\vert x_{1i} - x_{2i} \right\vert^{2})^{\frac{1}{2}}
\end{aligned}
$$

$d$ 값이 작을 수록 거리가 가깝다고 판단하게 되고 가장 가까운 $k$개의 데이터로부터 종속변수의 값을 예측하게 되는 것이다.

$\sqrt{}\$를 안씌워주어도 거리의 대소관계는 유지되기 때문에, 뒤에 나오는 파이썬 코드에선 연산속도 향상을 위해 $\sqrt{}\$를 씌워주지 않고, 거리 $d$를 다음과 같이 연산하였다.

$$
\begin{aligned}
d & = \sum_{i}^{n} \left\vert x_{1i} - x_{2i} \right\vert^{2} \\
& = (x_{11} - x_{21})^2 + (x_{12} - x_{22})^2 + \cdots + (x_{1n} - x_{2n})^2 \\
& = (x_{1} - x_{2})^{T} (x_{1} - x_{2})
\end{aligned}
$$

# 2. k-NN 분류

k-NN 분류는 종속변수가 이산형 변수 혹은 명목변수일 때 시행한다. 예측데이터로부터 가장 가까운 훈련데이터들의 $y$값(종속변수)에서 가장 빈도수가 높은 특성을 예측데이터의 특성으로 예측하는 것이다.

## 2.1. 파이썬 코드

k-NN 알고리즘을 파이썬 코드로 구현해보았다. fit 속성으로 훈련데이터를 적합시키고, predict 속성으로 특성을 예측하게 된다. score 속성은 모델의 성능(예측값과 실제값의 일치 비율)을 0과 1 사이에서 나타날 수 있게 하였다. $k$의 디폴트 값은 3으로 가장 가까운 3개의 훈련데이터를 이용해 예측을 한다.

```python
class knn_classifier:
    
    def __init__(self, k = 3):
        self.k = k
        
    def fit(self, X_train, y_train):
        self.X_train = X_train
        self.y_train = y_train
    
    def predict(self, X_test):
        self.X_test = X_test
        
        y_pred = []
        
        for test_point in self.X_test:
            diff_square_list = []
            
            for train_point in self.X_train:
                diff_vector = test_point - train_point
                diff_square = np.dot(diff_vector, diff_vector)
                diff_square_list.append(diff_square)
            
            diff_square_list = np.array(diff_square_list)
            near_idx = np.argsort(diff_square_list)
            
            label = self.y_train[near_idx][:self.k]
            label_idx, label_count = np.unique(label, return_counts = True)
            most_counts_label = label_idx[np.argmax(label_count)]
            y_pred.append(most_counts_label)
        
        return np.array(y_pred)
    
    def score(self, X_test, y_test):
        
        return np.mean(self.predict(X_test) == y_test)
```

k-NN 모델의 예측과정을 시각적으로 나타내기 위해 sklearn 모듈의 make_blobs 데이터셋을 가져왔다.

```python
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_blobs
from sklearn.model_selection import train_test_split

np.random.seed(0)

X, y = make_blobs(n_samples = 200, centers = 2, n_features=2, random_state=0, cluster_std = 0.4)

plt.scatter(X[:, 0], X[:, 1], c = y, cmap = 'winter', s = 100, alpha = 0.5)
plt.xlabel('$x_{1}$')
plt.ylabel('$x_{2}$')
plt.grid()
```

![2020-02-14-scatter_plot](/assets/img/2020-02-14-scatter_plot.png)

데이터의 개수는 200개이다. $x$축에는 $x_{1}$ 속성, $y$축에는 $x_{2}$속성으로 독립변수를 표현하였고, 종속변수로 0'과 '1'의 특성을 갖는 $y$값을 각각 파란색과 초록색으로 표현하였다.

```python
X_train, X_test, y_train, y_test = train_test_split(X, y)

plt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, cmap = 'winter', label = 'train data', s = 100, alpha = 0.3)
plt.scatter(X_test[:, 0], X_test[:, 1], c = 'gray', label = 'test data', s = 200, marker = '*')
plt.legend()
plt.xlabel('$x_{1}$')
plt.ylabel('$x_{2}$')
plt.grid()
```

![2020-02-14-scatter_plot_with_pred1](/assets/img/2020-02-14-scatter_plot_with_pred1.png)

sklearn의 train_test_split() 함수를 이용하여 200개의 데이터를 150개의 학습데이터와 50개의 시험데이터(회색 별)로 분류하였다.

이제 150개의 학습데이터들을 knn_classifier 모델에 학습시켜 50개의 시험데이터(회색 별)의 특성(파란색 or 초록색)을 예측할 것이다.

```python
knn = knn_classifier() # 위에서 만든 지도학습 알고리즘
knn.fit(X_train, y_train) # train data 적합
y_pred = knn.predict(X_test) # 예측 레이블 반환
print('예측 스코어 : ' + str(knn.score(X_test, y_test))) # 예측률 반환

plt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, cmap = 'winter', label = 'train data', s = 100, alpha = 0.3)
plt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred, cmap = 'winter', label = 'test data', s = 200, marker = '*')
plt.legend()
plt.xlabel('$x_{1}$')
plt.ylabel('$x_{2}$')
plt.grid()
```

![2020-02-14-scatter_plot_with_pred2](/assets/img/2020-02-14-scatter_plot_with_pred2.png)

knn 분류기가 디폴트 값인 $k$ = 3에서 분류를 잘 해낸것을 확인 할 수 있다. 특성값을 알 수 없던 회색 별의 색을 파란색(0) 혹은 초록색(1)으로 잘 맞추었다. 예측스코어는 1.0이 나왔고 이는 예측값과 실제값이 모두 일치해 예측이 모두 성공했음을 의미한다.

# 3. k-NN 회귀 

k-NN 회귀는 종속변수가 연속형 변수일 때 시행한다. 예측데이터로 부터 가장 가까운 $k$개의 훈련데이터들을 뽑아 $y$값(종속변수)들의 평균을 예측데이터의 $y$값으로 예측하게 된다.

$k$의 값이 작으면 작을수록 **과대적합(Overfitting)**, $k$의 개수가 커질수록 **과소적합(Underfitting)**이 된다. 적절한 $k$값을 찾는게 k-NN 회귀 알고리즘의 핵심이다.

## 3.1 파이썬 코드

```python
import numpy as np

class knn_regressor:
    
    def __init__(self, k = 3):
        self.k = k
        
    def fit(self, X_train, y_train):
        self.X_train = X_train
        self.y_train = y_train
    
    def predict(self, X_test):
        self.X_test = X_test
        
        y_pred = []
        
        for test_point in self.X_test:
            diff_square_list = []
            
            for train_point in self.X_train:
                diff_vector = test_point - train_point
                diff_square = np.dot(diff_vector, diff_vector)
                diff_square_list.append(diff_square)
            
            diff_square_list = np.array(diff_square_list)
            near_idx = np.argsort(diff_square_list)
            
            y_pred.append(np.mean(self.y_train[near_idx][:self.k]))
        
        return np.array(y_pred)
```

회귀 알고리즘은 분류 알고리즘과 대동소이하다. 이 역시 모델의 예측과정을 시각화 하기위해 mglearn모듈의 make_wave 데이터셋을 가져왔다.

```python
x, y = make_wave(n_samples = 100)

scatter_plot_wave = plt.figure(figsize = (8, 6))
plt.scatter(x, y, label = 'data point')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid()
scatter_plot_wave.savefig('2020-02-14-scatter_plot_wave.png', dpi = 200)
```

![2020-02-14-scatter_plot_wave](/assets/img/2020-02-14-scatter_plot_wave.png)

$x$ 축에는 독립변수를, $y$축에는 종속변수를 나타내었고, 데이터의 경향은 굴곡이 굽이치는 것을 확인 할 수 있다. $k$ 값을 변화시키면서 knn_regressor 모델로 예측한 회귀선이 어떤 모양을 갖는지 확인 할 것이다.

```python
x_test = np.arange(-3, 3, 0.05)

# k = 3
knn3 = knn_regressor()
knn3.fit(x, y)
y_pred3 = knn3.predict(x_test)

# k = 10
knn10 = knn_regressor(k = 10)
knn10.fit(x, y)
y_pred10 = knn10.predict(x_test)

# k = 30
knn30 = knn_regressor(k = 30)
knn30.fit(x, y)
y_pred30 = knn30.predict(x_test)

# k = 100
knn100 = knn_regressor(k = 100)
knn100.fit(x, y)
y_pred100 = knn100.predict(x_test)

# plot 생성
scatter_plot_wave_with_regressor_line = plt.figure(figsize = (8, 6))

plt.subplot(221)
plt.scatter(x, y, label = 'data point')
plt.plot(x_test, y_pred3, c = 'red', label = 'regressor line')
plt.title('k = 3')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid()

plt.subplot(222)
plt.scatter(x, y, label = 'data point')
plt.plot(x_test, y_pred10, c = 'red', label = 'regressor line')
plt.title('k = 10')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid()

plt.subplot(223)
plt.scatter(x, y, label = 'data point')
plt.plot(x_test, y_pred30, c = 'red', label = 'regressor line')
plt.title('k = 30')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid()

plt.subplot(224)
plt.scatter(x, y, label = 'data point')
plt.plot(x_test, y_pred100, c = 'red', label = 'regressor line')
plt.title('k = 100')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid()

plt.tight_layout()
scatter_plot_wave_with_regressor_line.savefig('2020-02-14-scatter_plot_wave_with_regressor_line.png', dpi = 200)
```

![2020-02-14-scatter_plot_wave_with_regressor_line](/assets/img/2020-02-14-scatter_plot_wave_with_regressor_line.png)

$k$ 값을 3, 10, 30, 100으로 변화시키면서 회귀선의 변화를 관찰해보았다. $k=3$일 때는 **과대적합**되어 회귀선이 매우 불안정한 것을 확인할 수 있다. 반면 $k = 100$일 때는 전체 데이터의 개수가 100개이므로 회귀선 전체가 데이터 전체의 평균값을 가져 **과소적합**이 되어 있는 것을 확인할 수 있다.